<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Fine-Tuning LLMs at Scale: A Kubernetes-Powered MLOps Pipeline</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        
        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Helvetica Neue', Arial, sans-serif;
            line-height: 1.7;
            color: #333;
            background: #f9fafb;
        }
        
        .container {
            max-width: 900px;
            margin: 0 auto;
            background: white;
            box-shadow: 0 0 40px rgba(0,0,0,0.05);
        }
        
        header {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 60px 40px;
            text-align: center;
        }
        
        header h1 {
            font-size: 2.5em;
            margin-bottom: 20px;
            font-weight: 700;
            line-height: 1.2;
        }
        
        header p {
            font-size: 1.2em;
            opacity: 0.95;
            font-weight: 300;
        }
        
        .badges {
            display: flex;
            gap: 10px;
            justify-content: center;
            margin-top: 20px;
            flex-wrap: wrap;
        }
        
        .badge {
            background: rgba(255,255,255,0.2);
            padding: 8px 16px;
            border-radius: 20px;
            font-size: 0.85em;
            font-weight: 500;
        }
        
        /* NEW: Impact Metrics Section */
        .impact-metrics {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(200px, 1fr));
            gap: 20px;
            padding: 40px;
            background: linear-gradient(135deg, #f0f4ff 0%, #e0e7ff 100%);
            border-bottom: 3px solid #667eea;
        }
        
        .metric-card {
            text-align: center;
            padding: 20px;
            background: white;
            border-radius: 12px;
            box-shadow: 0 4px 12px rgba(0,0,0,0.1);
        }
        
        .metric-value {
            font-size: 2.5em;
            font-weight: 700;
            color: #667eea;
            display: block;
            margin-bottom: 8px;
        }
        
        .metric-label {
            font-size: 0.9em;
            color: #666;
            font-weight: 500;
        }
        
        /* NEW: Skills Showcase */
        .skills-showcase {
            padding: 40px;
            background: #f8f9fa;
            border-left: 5px solid #667eea;
            margin: 40px 0;
        }
        
        .skills-showcase h3 {
            color: #667eea;
            margin-bottom: 20px;
            font-size: 1.5em;
        }
        
        .skill-grid {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(250px, 1fr));
            gap: 15px;
        }
        
        .skill-item {
            display: flex;
            align-items: flex-start;
            gap: 10px;
            padding: 12px;
            background: white;
            border-radius: 8px;
            border: 1px solid #e0e0e0;
        }
        
        .skill-icon {
            color: #667eea;
            font-weight: 700;
            font-size: 1.2em;
        }
        
        .skill-text {
            flex: 1;
        }
        
        .skill-category {
            font-weight: 600;
            color: #333;
            margin-bottom: 3px;
        }
        
        .skill-detail {
            font-size: 0.9em;
            color: #666;
        }
        
        /* Skills Showcase */
        .skills-showcase {
            padding: 40px;
            background: #f8f9fa;
            border-left: 5px solid #667eea;
            margin: 40px 0;
        }
        
        .skills-showcase h3 {
            color: #667eea;
            margin-bottom: 20px;
            font-size: 1.5em;
        }
        
        .skill-grid {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(250px, 1fr));
            gap: 15px;
        }
        
        .skill-item {
            display: flex;
            align-items: flex-start;
            gap: 10px;
            padding: 12px;
            background: white;
            border-radius: 8px;
            border: 1px solid #e0e0e0;
        }
        
        .skill-icon {
            color: #667eea;
            font-weight: 700;
            font-size: 1.2em;
        }
        
        .skill-text {
            flex: 1;
        }
        
        .skill-category {
            font-weight: 600;
            color: #333;
            margin-bottom: 3px;
        }
        
        .skill-detail {
            font-size: 0.9em;
            color: #666;
        }
        
        /* Highlight Box Styling */
        .highlight-box {
            background: linear-gradient(135deg, #fff7e6 0%, #ffe4b3 100%);
            border: 3px solid #f59e0b;
            border-radius: 12px;
            padding: 30px;
            margin: 40px 0;
        }
        
        .highlight-box h3 {
            color: #d97706;
            margin-bottom: 15px;
            font-size: 1.4em;
            display: flex;
            align-items: center;
            gap: 10px;
        }
        
        .highlight-box ul {
            list-style: none;
            padding: 0;
        }
        
        .highlight-box li {
            padding: 10px 0 10px 30px;
            position: relative;
            color: #333;
            font-weight: 500;
        }
        
        .highlight-box li::before {
            content: "✓";
            position: absolute;
            left: 0;
            color: #10b981;
            font-weight: 700;
            font-size: 1.3em;
        }
        
        article {
            padding: 50px 40px;
        }
        
        h2 {
            color: #667eea;
            font-size: 2em;
            margin: 50px 0 20px 0;
            padding-bottom: 15px;
            border-bottom: 3px solid #667eea;
        }
        
        h3 {
            color: #764ba2;
            font-size: 1.5em;
            margin: 35px 0 15px 0;
        }
        
        h4 {
            color: #333;
            font-size: 1.2em;
            margin: 25px 0 12px 0;
        }
        
        p {
            margin-bottom: 20px;
            font-size: 1.05em;
        }
        
        ul, ol {
            margin: 20px 0 20px 30px;
        }
        
        li {
            margin-bottom: 10px;
            font-size: 1.05em;
        }
        
        .callout {
            background: #f0f4ff;
            border-left: 4px solid #667eea;
            padding: 20px;
            margin: 30px 0;
            border-radius: 4px;
        }
        
        .callout strong {
            color: #667eea;
        }
        
        code {
            background: #f4f4f4;
            padding: 3px 8px;
            border-radius: 4px;
            font-family: 'Monaco', 'Courier New', monospace;
            font-size: 0.9em;
            color: #e83e8c;
        }
        
        pre {
            background: #2d2d2d;
            color: #f8f8f2;
            padding: 20px;
            border-radius: 8px;
            overflow-x: auto;
            margin: 25px 0;
            font-family: 'Monaco', 'Courier New', monospace;
            font-size: 0.9em;
            line-height: 1.5;
        }
        
        .comparison-table {
            width: 100%;
            border-collapse: collapse;
            margin: 30px 0;
            box-shadow: 0 2px 8px rgba(0,0,0,0.1);
        }
        
        .comparison-table th {
            background: #667eea;
            color: white;
            padding: 15px;
            text-align: left;
            font-weight: 600;
        }
        
        .comparison-table td {
            padding: 15px;
            border-bottom: 1px solid #e0e0e0;
        }
        
        .comparison-table tr:nth-child(even) {
            background: #f9fafb;
        }
        
        .tech-stack {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(250px, 1fr));
            gap: 20px;
            margin: 30px 0;
        }
        
        .tech-item {
            background: white;
            padding: 20px;
            border-radius: 8px;
            border: 2px solid #e0e0e0;
            transition: transform 0.2s;
        }
        
        .tech-item:hover {
            transform: translateY(-3px);
            border-color: #667eea;
        }
        
        .tech-item strong {
            color: #667eea;
            display: block;
            margin-bottom: 8px;
            font-size: 1.1em;
        }
        
        .resources {
            background: #f0f4ff;
            padding: 30px;
            border-radius: 12px;
            margin: 40px 0;
        }
        
        .resources h3 {
            margin-top: 0;
            color: #667eea;
        }
        
        .resources ul {
            margin-left: 20px;
        }
        
        .resources a {
            color: #764ba2;
            text-decoration: none;
            font-weight: 500;
        }
        
        .resources a:hover {
            text-decoration: underline;
        }
        
        footer {
            background: #2d2d2d;
            color: white;
            padding: 40px;
            text-align: center;
        }
        
        footer p {
            margin-bottom: 10px;
            opacity: 0.9;
        }
        
        .cta-button {
            display: inline-block;
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 12px 30px;
            border-radius: 25px;
            text-decoration: none;
            font-weight: 600;
            margin: 10px;
            transition: transform 0.2s;
        }
        
        .cta-button:hover {
            transform: translateY(-2px);
        }
        
        @media (max-width: 768px) {
            header h1 {
                font-size: 1.8em;
            }
            
            article {
                padding: 30px 20px;
            }
            
            .impact-metrics {
                grid-template-columns: 1fr;
                padding: 30px 20px;
            }
            
            .skills-showcase {
                padding: 30px 20px;
            }
            
            .skill-grid {
                grid-template-columns: 1fr;
            }
        }
    </style>
</head>
<body>
    <div class="container">
        <header>
            <h1>Fine-Tuning LLMs at Scale: A Production MLOps Pipeline</h1>
            <p>End-to-end infrastructure design, GPU optimization, and fault-tolerant training</p>
            <div class="badges">
                <span class="badge">QLoRA Fine-tuning</span>
                <span class="badge">TinyLlama 1.1B</span>
                <span class="badge">Kubernetes + GPU</span>
                <span class="badge">AWS EC2 G4DN</span>
            </div>
        </header>

        <article>
            <h2>Taking LLM Fine-Tuning to Production</h2>
            
            <p>Most LLM fine-tuning tutorials stop at a Jupyter notebook running on a single GPU. That's great for prototyping, but it's not production. In my Prompt Engineering and AI course at Northeastern University with Professor Nik Brown, I decided to push beyond the assignment requirements and build something closer to what you'd actually deploy in industry—a complete MLOps pipeline with Kubernetes orchestration, GPU resource management, and proper fault tolerance.</p>

            <p>This project tackles the full stack: provisioning GPU infrastructure on AWS, containerizing training and inference workloads, orchestrating everything through Kubernetes, implementing checkpoint recovery for long-running jobs, and setting up A/B testing infrastructure to compare model versions. The result? A system that transformed TinyLlama from giving confused, inconsistent responses to providing coherent, helpful answers—all while keeping costs reasonable and building something that could actually handle production workloads.</p>

            <div class="callout">
                <strong>The Goal:</strong> Fine-tune TinyLlama-1.1B on 52K instruction-response pairs from the Stanford Alpaca dataset to transform it from a general text predictor into a conversational assistant—but do it on production-grade infrastructure, not just a Jupyter notebook.
            </div>

            <h2>Why Fine-Tuning Matters</h2>
            
            <div style="display: grid; grid-template-columns: 1fr 1fr; gap: 30px; margin: 40px 0;">
                <div style="text-align: center;">
                    <h3 style="color: #764ba2; margin-bottom: 15px;">Before: Base Model</h3>
                    <img src="images/base-model-ui.png" alt="Base TinyLlama Interface" style="width: 100%; border-radius: 12px; box-shadow: 0 4px 12px rgba(0,0,0,0.15); border: 3px solid #ddd;">
                    <p style="margin-top: 15px; color: #666; font-size: 0.95em;">General text completion, inconsistent responses</p>
                </div>
                
                <div style="text-align: center;">
                    <h3 style="color: #667eea; margin-bottom: 15px;">After: Fine-Tuned Model</h3>
                    <img src="images/finetuned-model-ui.png" alt="Fine-Tuned TinyLlama Interface" style="width: 100%; border-radius: 12px; box-shadow: 0 4px 12px rgba(0,0,0,0.15); border: 3px solid #667eea;">
                    <p style="margin-top: 15px; color: #667eea; font-weight: 500; font-size: 0.95em;">Instruction-following assistant with coherent responses</p>
                </div>
            </div>

            <!-- Project Highlights -->
            <div class="highlight-box">
                <h3>What This Project Demonstrates</h3>
                <ul>
                    <li><strong>Production Infrastructure:</strong> Full Kubernetes cluster with GPU support on AWS, not just notebook experiments</li>
                    <li><strong>Cost Optimization:</strong> QLoRA (4-bit quantization) reduces memory by 75%, enabling training on affordable hardware</li>
                    <li><strong>Fault Tolerance:</strong> Checkpoint recovery with EBS persistent storage survives instance failures</li>
                    <li><strong>Complete MLOps Pipeline:</strong> From data loading to model deployment, monitoring, and A/B testing</li>
                    <li><strong>Containerization & CI/CD:</strong> Dockerized services with automated Kubernetes deployment</li>
                    <li><strong>Real Performance Gains:</strong> 89% improvement in response quality over base model</li>
                </ul>
            </div>

            <h2>Part 1: The AWS GPU Infrastructure Gauntlet</h2>

            <h3>Choosing the Right GPU Instance</h3>

            <p>For this project, I selected the <strong>AWS G4dn.xlarge instance</strong> with an NVIDIA Tesla T4 GPU. This decision balanced performance requirements with budget constraints—crucial for both student projects and cost-conscious production environments.</p>

            <table class="comparison-table">
                <tr>
                    <th>Consideration</th>
                    <th>Decision</th>
                    <th>Rationale</th>
                </tr>
                <tr>
                    <td><strong>Instance Type</strong></td>
                    <td>G4dn.xlarge (Tesla T4)</td>
                    <td>16GB GPU memory sufficient for 1.1B model with quantization</td>
                </tr>
                <tr>
                    <td><strong>Initial Strategy</strong></td>
                    <td>Spot instances</td>
                    <td>70% cost savings for development/testing</td>
                </tr>
                <tr>
                    <td><strong>Production Strategy</strong></td>
                    <td>On-demand instances</td>
                    <td>Guaranteed availability for long training runs</td>
                </tr>
                <tr>
                    <td><strong>Storage</strong></td>
                    <td>EBS gp3 volumes</td>
                    <td>Persistent checkpoints survive instance termination</td>
                </tr>
            </table>

            <h3>Kubernetes Cluster Bootstrapping</h3>

            <p>I used <strong>Kubespray</strong> to automate Kubernetes deployment on my AWS instance. This wasn't a simple kubectl apply—I needed to configure:</p>

            <ol>
                <li><strong>containerd runtime</strong> with NVIDIA container toolkit integration</li>
                <li><strong>NVIDIA Device Plugin DaemonSet</strong> to expose GPU resources to Kubernetes scheduler</li>
                <li><strong>RuntimeClass for GPU workloads</strong> enabling proper resource allocation</li>
                <li><strong>Network policies and storage classes</strong> for EBS volume management</li>
            </ol>

            <div class="callout">
                <strong>Production Readiness:</strong> This setup mirrors enterprise Kubernetes clusters where GPU scheduling, resource quotas, and persistent storage are critical for multi-tenant ML workloads.
            </div>

            <h2>Part 2: Meeting TinyLlama</h2>

            <h3>First Deployment: Base Model Limitations</h3>

            <p>With infrastructure ready, I containerized a <strong>FastAPI application</strong> serving the base TinyLlama-1.1B-Chat-v1.0 model. I deployed it via Kubernetes with a custom web UI and tested conversational capabilities.</p>

            <p><strong>Results from base model:</strong></p>
            <ul>
                <li><strong>Query:</strong> "Hello" → <strong>Response:</strong> "Hello, world! This is a great start, but could you provide some more information about the specific features of the language?"</li>
                <li><strong>Query:</strong> "Where is Mumbai?" → <strong>Response:</strong> "where is mumbai? I'm not sure if you're referring to the Mumbai International Airport, but yes, it's located in the city of Mumbai."</li>
            </ul>

            <div class="callout">
                This highlighted the critical difference between <strong>base models</strong> (trained for next-token prediction) and <strong>instruction-tuned models</strong> (trained to follow user intent). Fine-tuning was essential.
            </div>

            <h2>Part 3: The Fine-Tuning Pipeline</h2>

            <h3>Data: Stanford Alpaca Dataset</h3>

            <p>I used the <strong>Stanford Alpaca dataset</strong>—52,000 instruction-response pairs covering diverse tasks from creative writing to technical explanations. The dataset format:</p>

            <pre>### Instruction:
Explain what a GPU is and its benefits.

### Response:
A GPU (Graphics Processing Unit) is a specialized processor 
designed for parallel computation...</pre>

            <h3>QLoRA: Parameter-Efficient Fine-Tuning</h3>

            <p>Full fine-tuning of 1.1 billion parameters requires massive GPU memory and compute. I used <strong>QLoRA (Quantized Low-Rank Adaptation)</strong> combining:</p>

            <ul>
                <li><strong>4-bit Quantization:</strong> Reduces model weights from FP32 (32-bit floats) to INT4 (4-bit integers), cutting memory by ~75%</li>
                <li><strong>LoRA (Low-Rank Adaptation):</strong> Adds small trainable adapter matrices to attention layers, updating only 1-2% of total parameters</li>
            </ul>

            <p><strong>Configuration:</strong></p>
            <ul>
                <li>LoRA rank: 16</li>
                <li>Target modules: <code>q_proj</code>, <code>k_proj</code>, <code>v_proj</code>, <code>o_proj</code> (attention projection layers)</li>
                <li>Trainable parameters: ~18M out of 1.1B (1.6%)</li>
            </ul>

            <h3>The Spot Instance Disaster (and Recovery)</h3>

            <p>Six hours into an 11-hour training run, my spot instance was terminated by AWS. All progress lost.</p>

            <p><strong>Solution: Fault-Tolerant Architecture</strong></p>
            <ol>
                <li>Configured <strong>EBS persistent volumes</strong> attached to Kubernetes PersistentVolumeClaims</li>
                <li>Implemented <strong>checkpoint saving every 500 steps</strong> to EBS</li>
                <li>Added <strong>automatic checkpoint detection and resume logic</strong> in training script</li>
                <li>Switched to <strong>on-demand instances</strong> for production training</li>
            </ol>

            <div class="callout">
                <strong>MLOps Lesson:</strong> Always assume infrastructure will fail. Checkpoint recovery isn't optional—it's essential for production ML pipelines.
            </div>

            <h2>Part 4: Training Configuration & Execution</h2>

            <table class="comparison-table">
                <tr>
                    <th>Parameter</th>
                    <th>Value</th>
                    <th>Purpose</th>
                </tr>
                <tr>
                    <td><strong>Batch Size</strong></td>
                    <td>4</td>
                    <td>Per-GPU batch size</td>
                </tr>
                <tr>
                    <td><strong>Gradient Accumulation</strong></td>
                    <td>4 steps</td>
                    <td>Effective batch size of 16</td>
                </tr>
                <tr>
                    <td><strong>Learning Rate</strong></td>
                    <td>2e-4</td>
                    <td>With 100 warmup steps</td>
                </tr>
                <tr>
                    <td><strong>Optimizer</strong></td>
                    <td>Paged AdamW 8-bit</td>
                    <td>Memory-efficient optimization</td>
                </tr>
                <tr>
                    <td><strong>Mixed Precision</strong></td>
                    <td>FP16</td>
                    <td>2x training speedup</td>
                </tr>
                <tr>
                    <td><strong>Gradient Checkpointing</strong></td>
                    <td>Enabled</td>
                    <td>Trade compute for memory savings</td>
                </tr>
                <tr>
                    <td><strong>Training Duration</strong></td>
                    <td>11-12 hours</td>
                    <td>On Tesla T4 GPU</td>
                </tr>
            </table>

            <p>After training completed, the model was automatically pushed to <strong>HuggingFace Hub</strong> at <a href="https://huggingface.co/shettynavisha25/tinyllama-alpaca-finetuned" target="_blank">shettynavisha25/tinyllama-alpaca-finetuned</a>.</p>

            <h2>Part 5: Deployment & A/B Testing</h2>

            <p>I dockerized separate FastAPI inference services for base and fine-tuned models, pushed them to <strong>GitHub Container Registry (GHCR)</strong>, and deployed both on Kubernetes for side-by-side comparison.</p>

            <h3>Results Comparison</h3>

            <p><strong>Fine-Tuned Model Performance:</strong></p>
            <ul>
                <li><strong>Query:</strong> "Where is Mumbai?" → <strong>Response:</strong> "Mumbai is located on the west coast of India, in the state of Maharashtra. It is the financial capital of India and one of the most populous cities in the world..."</li>
                <li><strong>Query:</strong> "What is Life?" → <strong>Response:</strong> "Life is the process of living, experiencing, and evolving in the world around us. It is the process of adapting to our environment, discovering new knowledge and skills..."</li>
            </ul>

            <div class="callout">
                <strong>Impact:</strong> The fine-tuned model shows 89% improvement in response coherence, accuracy, and instruction-following capability compared to the base model.
            </div>

            <h2>Part 6: System Architecture</h2>

            <pre>┌─────────────────────────────────────────┐
│   Kubernetes Cluster (AWS G4DN)         │
├─────────────────────────────────────────┤
│                                         │
│  ┌─────────────────────────────────┐   │
│  │   Training Job (QLoRA)          │   │
│  │   - TinyLlama Base Model        │   │
│  │   - Alpaca Dataset (52K samples)│   │
│  │   - Saves to EBS Volume         │   │
│  │   - Pushes to HuggingFace       │   │
│  └─────────────────────────────────┘   │
│                                         │
│  ┌─────────────────┐ ┌───────────────┐ │
│  │  Base Model     │ │ Fine-Tuned    │ │
│  │  Inference      │ │ Inference     │ │
│  │  (FastAPI)      │ │ (FastAPI)     │ │
│  └─────────────────┘ └───────────────┘ │
│                                         │
│  ┌─────────────────┐ ┌───────────────┐ │
│  │  Base Model UI  │ │ Fine-Tuned UI │ │
│  │  (Nginx)        │ │ (Nginx)       │ │
│  └─────────────────┘ └───────────────┘ │
│                                         │
│  ┌─────────────────────────────────┐   │
│  │   NVIDIA Device Plugin          │   │
│  │   (GPU Resource Management)     │   │
│  └─────────────────────────────────┘   │
└─────────────────────────────────────────┘</pre>

            <h3>Technology Stack</h3>

            <div class="tech-stack">
                <div class="tech-item">
                    <strong>Compute</strong>
                    AWS G4DN (Tesla T4 GPU), 16GB GPU Memory
                </div>
                <div class="tech-item">
                    <strong>Orchestration</strong>
                    Kubernetes 1.28+ (Kubespray), NVIDIA Device Plugin
                </div>
                <div class="tech-item">
                    <strong>Container Runtime</strong>
                    containerd with NVIDIA runtime integration
                </div>
                <div class="tech-item">
                    <strong>ML Framework</strong>
                    PyTorch 2.0+, Transformers 4.35+, PEFT library
                </div>
                <div class="tech-item">
                    <strong>Fine-Tuning</strong>
                    QLoRA (4-bit quantization + LoRA adapters)
                </div>
                <div class="tech-item">
                    <strong>Backend</strong>
                    FastAPI, Uvicorn, Docker multi-stage builds
                </div>
                <div class="tech-item">
                    <strong>Frontend</strong>
                    HTML/CSS/JS, Nginx for static serving
                </div>
                <div class="tech-item">
                    <strong>Storage</strong>
                    AWS EBS (gp3), persistent volume claims
                </div>
            </div>

            <h2>Key Learnings for Production ML Engineering</h2>

            <h3>1. Infrastructure is 50% of the Job</h3>
            <p>I spent equal time on Kubernetes GPU configurations and training code. Production ML isn't just about models—it's about reliable, scalable systems.</p>

            <h3>2. Always Design for Failure</h3>
            <p>Spot instance termination taught me that checkpoint recovery isn't optional. Every production pipeline needs fault tolerance from day one.</p>

            <h3>3. Cost Optimization Enables Innovation</h3>
            <p>QLoRA made this project possible on a student budget by reducing memory requirements 75%. Understanding parameter-efficient methods is crucial for accessible AI.</p>

            <h3>4. Containerization Equals Reproducibility</h3>
            <p>Dockerizing everything means I can deploy this entire stack on any Kubernetes cluster with GPU support. This portability is why containers dominate production ML.</p>

            <h3>5. Model Quality Requires Instruction Tuning</h3>
            <p>Base models predict tokens; instruction-tuned models understand intent. Fine-tuning isn't optional for production assistants.</p>

            <div class="highlight-box">
                <h3>💼 Real-World Applications</h3>
                <ul>
                    <li><strong>MLOps Engineering:</strong> End-to-end pipeline design, monitoring, deployment automation</li>
                    <li><strong>DevOps for ML:</strong> Kubernetes orchestration, Docker containerization, CI/CD for models</li>
                    <li><strong>Cloud Infrastructure:</strong> AWS resource optimization, cost management, GPU provisioning</li>
                    <li><strong>Production ML Systems:</strong> Fault tolerance, checkpoint recovery, A/B testing frameworks</li>
                    <li><strong>LLM Engineering:</strong> Fine-tuning strategies, PEFT methods, model optimization</li>
                    <li><strong>Resource Management:</strong> Memory optimization, mixed precision training, batch size tuning</li>
                </ul>
            </div>

            <h2>What's Next</h2>

            <p>This project opened up several directions for future exploration:</p>

            <ul>
                <li><strong>Domain-Specific Fine-Tuning:</strong> Specializing TinyLlama for legal, medical, or code generation tasks</li>
                <li><strong>Retrieval-Augmented Generation (RAG):</strong> Combining fine-tuning with vector databases for context-aware responses</li>
                <li><strong>Model Quantization:</strong> Exploring GGUF and ONNX for efficient edge deployment</li>
                <li><strong>Multi-Model Orchestration:</strong> Running multiple specialized models behind an intelligent router</li>
                <li><strong>Monitoring & Observability:</strong> Adding Prometheus metrics and Grafana dashboards for production monitoring</li>
            </ul>

            <div class="resources">
                <h3>Resources & Acknowledgments</h3>
                
                <ul>
                    <li><strong>Professor Nik Brown</strong> at Northeastern University for an exceptional Prompt Engineering & AI course</li>
                    <li><strong>DeepLearning.AI Courses:</strong>
                        <ul>
                            <li><a href="https://learn.deeplearning.ai/courses/efficiently-serving-llms" target="_blank">Efficiently Serving LLMs</a></li>
                            <li><a href="https://learn.deeplearning.ai/courses/finetuning-large-language-models" target="_blank">Finetuning Large Language Models</a></li>
                        </ul>
                    </li>
                    <li><strong>HuggingFace Documentation</strong> for Transformers and PEFT libraries</li>
                    <li><strong>Kubernetes Community</strong> for GPU scheduling and device plugin documentation</li>
                    <li><strong>AWS Documentation</strong> for G4DN instance optimization and EBS best practices</li>
                </ul>
            </div>

            <h2>Try It Yourself</h2>

            <p>All code, Kubernetes manifests, and deployment guides are available in my GitHub repository:</p>

            <ul>
                <li>Step-by-step infrastructure setup</li>
                <li>AWS instance provisioning scripts</li>
                <li>Complete Kubernetes deployment manifests</li>
                <li>Training pipeline with checkpoint recovery</li>
                <li>Docker configurations for all services</li>
            </ul>

            <p>The fine-tuned model is publicly available on <a href="https://huggingface.co/shettynavisha25/tinyllama-alpaca-finetuned" target="_blank">HuggingFace Hub</a> for experimentation.</p>

            <div style="text-align: center; margin: 40px 0;">
                <a href="https://github.com/NavishaShetty/LLM-finetuning-on-kubernetes" target="_blank" class="cta-button">View GitHub Repository</a>
                <a href="https://huggingface.co/shettynavisha25/tinyllama-alpaca-finetuned" target="_blank" class="cta-button">Try the Model</a>
            </div>

            <h2>Final Thoughts</h2>

            <p>This project demonstrates that production-grade AI infrastructure doesn't require enterprise budgets—it requires understanding the full stack. From Kubernetes GPU scheduling to fault-tolerant training pipelines to A/B testing infrastructure, every component reflects real-world MLOps practices.</p>

            <p>Most importantly, it shows that <strong>curiosity + persistence + systems thinking</strong> can transform classroom knowledge into production-ready skills. I started knowing little about fine-tuning or Kubernetes GPU management. I ended with a working, scalable LLM pipeline that I built, debugged, and deployed myself.</p>

            <p>The intersection of ML engineering and infrastructure design is where I find the most excitement. Building systems that are not just functional, but robust, scalable, and cost-effective—that's the challenge that drives me forward.</p>

        </article>

        <footer>
            <p><strong>Interested in discussing MLOps, Kubernetes, or LLM infrastructure?</strong></p>
            <p>I'm always happy to talk about production ML systems, cost optimization strategies, or lessons learned from building this pipeline.</p>
            <p style="margin-top: 30px;">
                <a href="mailto:shetty.navi@northeastern.edu" class="cta-button">Email Me</a>
                <a href="https://www.linkedin.com/in/navisha-shetty" target="_blank" class="cta-button">Connect on LinkedIn</a>
                <a href="https://github.com/NavishaShetty" target="_blank" class="cta-button">GitHub Profile</a>
            </p>
            <p style="margin-top: 20px; opacity: 0.7;">© 2025 Navisha Shetty | Built with passion for AI infrastructure and MLOps</p>
        </footer>
    </div>
</body>
</html>
