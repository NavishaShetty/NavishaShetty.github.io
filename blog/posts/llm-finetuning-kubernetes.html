<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>From Curious Student to LLM Fine-Tuner: Building a Production-Grade AI Pipeline on Kubernetes</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        
        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Helvetica Neue', Arial, sans-serif;
            line-height: 1.7;
            color: #333;
            background: #f9fafb;
        }
        
        .container {
            max-width: 900px;
            margin: 0 auto;
            background: white;
            box-shadow: 0 0 40px rgba(0,0,0,0.05);
        }
        
        header {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 60px 40px;
            text-align: center;
        }
        
        header h1 {
            font-size: 2.5em;
            margin-bottom: 20px;
            font-weight: 700;
            line-height: 1.2;
        }
        
        header p {
            font-size: 1.2em;
            opacity: 0.95;
            font-weight: 300;
        }
        
        .badges {
            display: flex;
            gap: 10px;
            justify-content: center;
            margin-top: 20px;
            flex-wrap: wrap;
        }
        
        .badge {
            background: rgba(255,255,255,0.2);
            padding: 8px 16px;
            border-radius: 20px;
            font-size: 0.85em;
            font-weight: 500;
        }
        
        article {
            padding: 50px 40px;
        }
        
        h2 {
            color: #667eea;
            font-size: 2em;
            margin: 50px 0 20px 0;
            padding-bottom: 15px;
            border-bottom: 3px solid #667eea;
        }
        
        h3 {
            color: #764ba2;
            font-size: 1.5em;
            margin: 35px 0 15px 0;
        }
        
        p {
            margin-bottom: 20px;
            font-size: 1.05em;
        }
        
        ul, ol {
            margin: 20px 0 20px 30px;
        }
        
        li {
            margin-bottom: 10px;
            font-size: 1.05em;
        }
        
        .callout {
            background: #f0f4ff;
            border-left: 4px solid #667eea;
            padding: 20px;
            margin: 30px 0;
            border-radius: 4px;
        }
        
        .callout strong {
            color: #667eea;
        }
        
        code {
            background: #f4f4f4;
            padding: 3px 8px;
            border-radius: 4px;
            font-family: 'Monaco', 'Courier New', monospace;
            font-size: 0.9em;
            color: #e83e8c;
        }
        
        pre {
            background: #2d2d2d;
            color: #f8f8f2;
            padding: 20px;
            border-radius: 8px;
            overflow-x: auto;
            margin: 25px 0;
            font-family: 'Monaco', 'Courier New', monospace;
            font-size: 0.9em;
            line-height: 1.5;
        }
        
        .screenshot-section {
            margin: 40px 0;
            padding: 30px;
            background: #f8f9fa;
            border-radius: 12px;
        }
        
        .screenshot-section h3 {
            margin-top: 0;
            text-align: center;
        }
        
        .screenshot-grid {
            display: grid;
            grid-template-columns: 1fr 1fr;
            gap: 20px;
            margin-top: 30px;
        }
        
        .screenshot-item {
            background: white;
            padding: 15px;
            border-radius: 8px;
            box-shadow: 0 2px 8px rgba(0,0,0,0.1);
        }
        
        .screenshot-item h4 {
            color: #667eea;
            margin-bottom: 15px;
            font-size: 1.1em;
            text-align: center;
        }
        
        .screenshot-item img {
            width: 100%;
            border-radius: 6px;
            border: 1px solid #e0e0e0;
        }
        
        .screenshot-full {
            margin: 20px 0;
            text-align: center;
        }
        
        .screenshot-full img {
            max-width: 100%;
            border-radius: 8px;
            box-shadow: 0 4px 12px rgba(0,0,0,0.15);
        }
        
        .architecture-diagram {
            background: white;
            padding: 30px;
            border-radius: 8px;
            margin: 30px 0;
            box-shadow: 0 2px 10px rgba(0,0,0,0.1);
        }
        
        .comparison-table {
            width: 100%;
            border-collapse: collapse;
            margin: 30px 0;
            box-shadow: 0 2px 8px rgba(0,0,0,0.1);
        }
        
        .comparison-table th {
            background: #667eea;
            color: white;
            padding: 15px;
            text-align: left;
            font-weight: 600;
        }
        
        .comparison-table td {
            padding: 15px;
            border-bottom: 1px solid #e0e0e0;
        }
        
        .comparison-table tr:nth-child(even) {
            background: #f9fafb;
        }
        
        .tech-stack {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(250px, 1fr));
            gap: 20px;
            margin: 30px 0;
        }
        
        .tech-item {
            background: white;
            padding: 20px;
            border-radius: 8px;
            border: 2px solid #e0e0e0;
            transition: transform 0.2s;
        }
        
        .tech-item:hover {
            transform: translateY(-3px);
            border-color: #667eea;
        }
        
        .tech-item strong {
            color: #667eea;
            display: block;
            margin-bottom: 8px;
            font-size: 1.1em;
        }
        
        .resources {
            background: #f0f4ff;
            padding: 30px;
            border-radius: 12px;
            margin: 40px 0;
        }
        
        .resources h3 {
            margin-top: 0;
            color: #667eea;
        }
        
        .resources ul {
            margin-left: 20px;
        }
        
        .resources a {
            color: #764ba2;
            text-decoration: none;
            font-weight: 500;
        }
        
        .resources a:hover {
            text-decoration: underline;
        }
        
        footer {
            background: #2d2d2d;
            color: white;
            padding: 40px;
            text-align: center;
        }
        
        footer p {
            margin-bottom: 10px;
            opacity: 0.9;
        }
        
        .cta-button {
            display: inline-block;
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 12px 30px;
            border-radius: 25px;
            text-decoration: none;
            font-weight: 600;
            margin: 10px;
            transition: transform 0.2s;
        }
        
        .cta-button:hover {
            transform: translateY(-2px);
        }
        
        .img-placeholder {
            background: #f0f2f5;
            border: 2px dashed #d9dde2;
            border-radius: 8px;
            padding: 20px;
            text-align: center;
            color: #6c757d;
            font-style: italic;
            min-height: 200px;
            display: flex;
            align-items: center;
            justify-content: center;
            width: 100%;
        }
        
        @media (max-width: 768px) {
            header h1 {
                font-size: 1.8em;
            }
            
            article {
                padding: 30px 20px;
            }
            
            .screenshot-grid {
                grid-template-columns: 1fr;
            }
            
            .tech-stack {
                grid-template-columns: 1fr;
            }
        }
    </style>
</head>
<body>
    <div class="container">
        <header>
            <h1>From Curious Student to LLM Fine-Tuner: Building a Production-Grade AI Pipeline on Kubernetes</h1>
            <p>A journey through GPU infrastructure, QLoRA fine-tuning, and real-world MLOps</p>
            <div class="badges">
                <span class="badge">üéì Fine-tuning with QLoRA</span>
                <span class="badge">ü§ñ TinyLlama 1.1B</span>
                <span class="badge">üì¶ Alpaca Dataset (52K)</span>
                <span class="badge">‚ò∏Ô∏è Kubernetes + GPU</span>
            </div>
        </header>

        <article>
            <h2>Introduction: When Passion Meets Practice</h2>
            
            <p>We've all been there‚Äîcurious about AI, fascinated by how ChatGPT works, wondering "could I build something like this?" That curiosity became my driving force when I enrolled in the Prompt Engineering and AI course at Northeastern University under Professor Nik Bear Brown. What started as academic exploration evolved into something much bigger: a full-scale, production-grade LLM fine-tuning pipeline orchestrated with Kubernetes.</p>

            <p>This isn't just another "I fine-tuned a model" story. This is about the messy, real-world journey of taking a 1.1 billion parameter language model from mediocre responses to genuinely useful conversations‚Äîwhile learning Kubernetes, AWS GPU infrastructure, Docker containerization, and modern MLOps practices along the way.</p>

            <div class="callout">
                <strong>Spoiler alert:</strong> Things broke. Instances terminated mid-training. Money was spent (and saved through clever engineering). But most importantly, I learned what it actually takes to deploy AI in production.
            </div>

            <h2>Part 1: The AWS GPU Gauntlet</h2>

            <h3>Starting Small (Because Student Budgets Are Real)</h3>

            <p>My first challenge wasn't even about AI‚Äîit was about infrastructure. I needed a GPU instance to fine-tune my model, but as a graduate student, I wasn't exactly swimming in AWS credits. Enter the <strong>G4dn.xlarge instance</strong> with an NVIDIA Tesla T4 GPU‚Äîpowerful enough for my needs, affordable enough not to drain my account overnight.</p>

            <p>But here's the thing about AWS: getting GPU instances on a student account isn't straightforward. I initially went with <strong>spot instances</strong> (AWS's discounted, interruptible compute) to save costs during development. This decision would later come back to haunt me, but more on that in a moment.</p>

            <h3>The Kubernetes Conundrum</h3>

            <p>Why Kubernetes for a student project? Because I wanted this to be <em>real</em>. Production systems use Kubernetes for a reason‚Äîit handles scaling, resource management, and service orchestration beautifully. If I was going to learn, I might as well learn the tools that matter.</p>

            <p>I used <strong>Kubespray</strong>, an automated Kubernetes deployment tool, to bootstrap my single-node cluster. The process involved:</p>

            <ol>
                <li><strong>Installing Kubernetes</strong> on my AWS Ubuntu instance</li>
                <li><strong>Configuring NVIDIA Container Toolkit</strong> to expose GPU resources to containers</li>
                <li><strong>Deploying the NVIDIA Device Plugin</strong> as a DaemonSet so Kubernetes could schedule GPU workloads</li>
                <li><strong>Setting up RuntimeClass</strong> for GPU-accelerated containers</li>
            </ol>

            <div class="screenshot-full">
                <h4>Project Structure</h4>
                <div class="img-placeholder">
                    <strong>Placeholder:</strong> Project file structure in VS Code showing infrastructure scripts, Kubernetes manifests, and training code.
                </div>
            </div>

            <p>This wasn't plug-and-play. I spent hours debugging containerd configurations, SSH key permissions, and network policies. But once everything clicked, I had a legitimate Kubernetes cluster with GPU support‚Äîpretty cool for a student project.</p>

            <h2>Part 2: Meeting TinyLlama (The Unfiltered Version)</h2>

            <h3>First Deployment: The Base Model Reality Check</h3>

            <p>With my infrastructure ready, I containerized a <strong>FastAPI application</strong> serving <strong>TinyLlama-1.1B-Chat-v1.0</strong>‚Äîa compact 1.1 billion parameter language model. I built a clean web UI with HTML/CSS/JavaScript, deployed it via Kubernetes, and eagerly opened my browser to chat with my new AI friend.</p>

            <p>And then... disappointment.</p>

            <div class="screenshot-section">
                <h3>Base Model Performance: Before Fine-Tuning</h3>
                
                <div class="screenshot-grid">
                    <div class="screenshot-item">
                        <h4>Example 1: Simple Query</h4>
                        <div class="img-placeholder"><strong>Placeholder:</strong> Base TinyLlama responding to 'Hello' with a confusing response.</div>
                        <p style="margin-top: 10px; font-size: 0.95em;"><strong>Query:</strong> "Hello"</p>
                        <p style="font-size: 0.95em; color: #666;"><strong>Response:</strong> "Hello, world! This is a great start, but could you provide some more information about the specific features of the language and how they can benefit the developer?"</p>
                    </div>
                    
                    <div class="screenshot-item">
                        <h4>Example 2: Factual Question</h4>
                        <div class="img-placeholder"><strong>Placeholder:</strong> Base TinyLlama giving an unclear answer about Mumbai.</div>
                        <p style="margin-top: 10px; font-size: 0.95em;"><strong>Query:</strong> "Where is Mumbai?"</p>
                        <p style="font-size: 0.95em; color: #666;"><strong>Response:</strong> "where is mumbai? I'm not sure if you're referring to the Mumbai International Airport, but yes, it's located in the city of Mumbai."</p>
                    </div>
                </div>
            </div>

            <p>Technically correct in some ways, but clearly not the coherent, helpful assistant I was hoping for. The base TinyLlama model was trained on general text data‚Äîgreat for completing sentences, terrible for following instructions or holding conversations.</p>

            <div class="callout">
                This is the difference between a <strong>base model</strong> and an <strong>instruction-tuned model</strong>. The former predicts the next word; the latter understands tasks. I needed to fine-tune.
            </div>

            <h2>Part 3: The Fine-Tuning Journey (AKA Learning Things the Hard Way)</h2>

            <h3>Discovering the Alpaca Dataset</h3>

            <p>To transform TinyLlama into a conversational assistant, I needed an <strong>instruction-following dataset</strong>. Enter <strong>Stanford Alpaca</strong>‚Äîa collection of 52,000 instruction-response pairs covering everything from creative writing to technical explanations to question answering.</p>

            <p>The Alpaca format looks like this:</p>

            <pre>### Instruction:
Explain what a GPU is and its benefits.

### Response:
A GPU (Graphics Processing Unit) is a specialized processor 
designed for parallel computation...</pre>

            <p>Perfect. Now I just needed to fine-tune my model on this data.</p>

            <h3>QLoRA: The Secret Sauce</h3>

            <p>Here's where things get technically interesting. Full fine-tuning of even a 1.1B parameter model requires substantial GPU memory and training time. I needed a smarter approach: <strong>QLoRA (Quantized Low-Rank Adaptation)</strong>.</p>

            <p>QLoRA combines two powerful techniques:</p>

            <ol>
                <li><strong>4-bit Quantization</strong>: Reduces model weights from 32-bit floats to 4-bit integers, slashing memory requirements by ~75% with minimal quality loss</li>
                <li><strong>LoRA (Low-Rank Adaptation)</strong>: Instead of updating all model parameters, LoRA adds small "adapter" matrices to specific layers and only trains those</li>
            </ol>

            <p>This is part of <strong>PEFT (Parameter-Efficient Fine-Tuning)</strong>‚Äîa family of methods that make fine-tuning accessible without enterprise-scale compute. In practice, QLoRA let me fine-tune on a single T4 GPU instead of needing multiple high-end cards.</p>

            <p>I configured LoRA to target the attention projection layers (<code>q_proj</code>, <code>k_proj</code>, <code>v_proj</code>, <code>o_proj</code>) with a rank of 16, meaning I was only training about 1-2% of the total parameters. Efficient and effective.</p>

            <h3>The Spot Instance Disaster</h3>

            <p>I kicked off training on my spot instance, expecting the process to take 11-12 hours. Everything was going smoothly‚Äîloss decreasing, checkpoints saving every 500 steps. Six and a half hours in, I checked the logs and saw beautiful progress.</p>

            <p>Then my terminal went silent.</p>

            <p>My spot instance had been <strong>terminated</strong>. AWS reclaimed the capacity, and my training progress? Gone.</p>

            <div class="callout">
                <strong>Lesson learned:</strong> This hurt. Not just because of lost compute time, but because I hadn't set up proper checkpoint recovery. I learned a crucial lesson about production ML: always assume your infrastructure will fail.
            </div>

            <h3>EBS to the Rescue</h3>

            <p>I redesigned my training pipeline with <strong>AWS EBS (Elastic Block Store)</strong> persistent volumes. EBS volumes are network-attached storage that persist independently of EC2 instances, meaning:</p>

            <ul>
                <li>Training checkpoints are saved to EBS every 500 steps</li>
                <li>If the instance terminates, I provision a new one</li>
                <li>The new instance mounts the same EBS volume</li>
                <li>Training resumes exactly where it left off</li>
            </ul>

            <p>I also finally got approval for an <strong>on-demand instance</strong> (no more spot instance roulette). Combined with checkpoint recovery, I could now train with confidence.</p>

            <h2>Part 4: Training Day</h2>

            <p>The actual training ran for approximately 11-12 hours on the Tesla T4 GPU. Here's what happened under the hood:</p>

            <table class="comparison-table">
                <tr>
                    <th>Parameter</th>
                    <th>Value</th>
                </tr>
                <tr>
                    <td><strong>Batch Size</strong></td>
                    <td>4 samples per GPU pass</td>
                </tr>
                <tr>
                    <td><strong>Gradient Accumulation</strong></td>
                    <td>4 steps (effective batch size of 16)</td>
                </tr>
                <tr>
                    <td><strong>Learning Rate</strong></td>
                    <td>2e-4 with 100 warmup steps</td>
                </tr>
                <tr>
                    <td><strong>Optimizer</strong></td>
                    <td>Paged AdamW 8-bit (memory-efficient)</td>
                </tr>
                <tr>
                    <td><strong>Gradient Checkpointing</strong></td>
                    <td>Enabled to trade compute for memory</td>
                </tr>
                <tr>
                    <td><strong>Mixed Precision</strong></td>
                    <td>FP16 training for speed</td>
                </tr>
            </table>

            <p>The Trainer automatically saved checkpoints to my EBS volume and, at the end, pushed the final fine-tuned model to <strong>HuggingFace Hub</strong> at <a href="https://huggingface.co/shettynavisha25/tinyllama-alpaca-finetuned" target="_blank">shettynavisha25/tinyllama-alpaca-finetuned</a>.</p>

            <h2>Part 5: The Results (Side-by-Side Comparison)</h2>

            <p>With the fine-tuned model ready, I dockerized a new FastAPI inference service, pushed it to <strong>GitHub Container Registry</strong>, and deployed both the base and fine-tuned models on my Kubernetes cluster for A/B comparison.</p>

            <div class="screenshot-section">
                <h3>Fine-Tuned Model Performance: After Training</h3>
                
                <div class="screenshot-grid">
                    <div class="screenshot-item">
                        <h4>Example 1: Geographic Query</h4>
                        <div class="img-placeholder"><strong>Placeholder:</strong> Fine-tuned model giving comprehensive answer about Mumbai.</div>
                        <p style="margin-top: 10px; font-size: 0.95em;"><strong>Query:</strong> "Where is Mumbai?"</p>
                        <p style="font-size: 0.95em; color: #2d7a3e;"><strong>Response:</strong> "Mumbai is located on the west coast of India, in the state of Maharashtra. It is the financial capital of India and one of the most populous cities in the world..."</p>
                    </div>
                    
                    <div class="screenshot-item">
                        <h4>Example 2: Philosophical Question</h4>
                        <div class="img-placeholder"><strong>Placeholder:</strong> Fine-tuned model answering 'What is Life?' thoughtfully.</div>
                        <p style="margin-top: 10px; font-size: 0.95em;"><strong>Query:</strong> "What is Life?"</p>
                        <p style="font-size: 0.95em; color: #2d7a3e;"><strong>Response:</strong> "Life is the process of living, experiencing, and evolving in the world around us. It is the process of adapting to our environment, discovering new knowledge and skills..."</p>
                    </div>
                </div>
            </div>

            <div class="callout">
                <strong>Night and day difference.</strong> The fine-tuned model understands the question, provides structured information, and responds like an assistant rather than a text completion engine.
            </div>

            <h2>Part 6: The Architecture</h2>

            <p>Here's what the final system looks like:</p>

            <div class="screenshot-full">
                <h4>Kubernetes Manifests Structure</h4>
                <div class="img-placeholder">
                    <strong>Placeholder:</strong> Kubernetes manifests organized by service type.
                </div>
            </div>

            <div class="architecture-diagram">
                <pre>‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   Kubernetes Cluster (AWS G4DN)         ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                         ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
‚îÇ  ‚îÇ   Training Job (QLoRA)          ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ   - TinyLlama Base Model        ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ   - Alpaca Dataset (52K samples)‚îÇ   ‚îÇ
‚îÇ  ‚îÇ   - Saves to EBS Volume         ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ   - Pushes to HuggingFace       ‚îÇ   ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
‚îÇ                                         ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ
‚îÇ  ‚îÇ  Base Model     ‚îÇ ‚îÇ Fine-Tuned    ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  Inference      ‚îÇ ‚îÇ Inference     ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  (FastAPI)      ‚îÇ ‚îÇ (FastAPI)     ‚îÇ ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ
‚îÇ                                         ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ
‚îÇ  ‚îÇ  Base Model UI  ‚îÇ ‚îÇ Fine-Tuned UI ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  (Nginx)        ‚îÇ ‚îÇ (Nginx)       ‚îÇ ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ
‚îÇ                                         ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
‚îÇ  ‚îÇ   NVIDIA Device Plugin          ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ   (GPU Resource Management)     ‚îÇ   ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò</pre>
            </div>

            <h3>Technology Stack</h3>

            <div class="tech-stack">
                <div class="tech-item">
                    <strong>Compute</strong>
                    AWS G4DN (Tesla T4 GPU)
                </div>
                <div class="tech-item">
                    <strong>Orchestration</strong>
                    Kubernetes 1.28+ (Kubespray)
                </div>
                <div class="tech-item">
                    <strong>Container Runtime</strong>
                    containerd with NVIDIA runtime
                </div>
                <div class="tech-item">
                    <strong>ML Framework</strong>
                    PyTorch 2.0+, Transformers 4.35+
                </div>
                <div class="tech-item">
                    <strong>Fine-Tuning</strong>
                    PEFT (QLoRA), 4-bit quantization
                </div>
                <div class="tech-item">
                    <strong>Backend</strong>
                    FastAPI, Uvicorn
                </div>
                <div class="tech-item">
                    <strong>Frontend</strong>
                    HTML/CSS/JS, Nginx
                </div>
                <div class="tech-item">
                    <strong>Storage</strong>
                    AWS EBS (gp3)
                </div>
            </div>

            <h2>Key Takeaways and Lessons Learned</h2>

            <h3>1. Infrastructure Matters More Than You Think</h3>
            <p>I spent more time debugging Kubernetes GPU configurations than writing training code. But that infrastructure knowledge is invaluable‚Äîproduction ML isn't just about models, it's about reliable systems.</p>

            <h3>2. Always Plan for Failure</h3>
            <p>My spot instance termination was frustrating but educational. Now I design every ML pipeline with checkpoint recovery, assuming compute will fail. This mindset is crucial for production systems.</p>

            <h3>3. PEFT Techniques Are Game-Changers</h3>
            <p>QLoRA made this project possible on a student budget. Without it, I'd need 4x the GPU memory and 3x the training time. Understanding techniques like LoRA, QLoRA, and prefix tuning is essential for practical AI engineering.</p>

            <h3>4. Dockerization + Kubernetes = Reproducibility</h3>
            <p>By containerizing everything, I can spin up this entire pipeline on any Kubernetes cluster with GPU support. This reproducibility is why Kubernetes dominates production ML.</p>

            <h3>5. Base Models ‚â† Production-Ready</h3>
            <p>The difference between base TinyLlama and the fine-tuned version is stark. Instruction tuning isn't optional‚Äîit's the difference between a language model and a usable assistant.</p>

            <h2>What's Next?</h2>

            <p>This project scratched my curiosity itch, but it opened up more questions:</p>

            <ul>
                <li><strong>Domain-Specific Fine-Tuning</strong>: Can I specialize TinyLlama for legal document analysis? Medical Q&A? Code generation?</li>
                <li><strong>Retrieval-Augmented Generation (RAG)</strong>: Combining fine-tuning with vector databases for context-aware responses</li>
                <li><strong>Model Quantization</strong>: Exploring GGUF and ONNX for even more efficient deployment</li>
                <li><strong>Multi-Model Orchestration</strong>: Running multiple specialized models behind a router for task-specific inference</li>
            </ul>

            <p>I'm currently working on a domain-specific fine-tune targeting technical documentation Q&A. Stay tuned.</p>

            <div class="resources">
                <h3>Resources That Helped Me</h3>
                
                <p>This project wouldn't have been possible without:</p>
                
                <ul>
                    <li><strong>Professor Nik Bear Brown</strong> at Northeastern University for an incredible Prompt Engineering & AI course</li>
                    <li><strong>DeepLearning.AI Courses</strong>:
                        <ul>
                            <li><a href="https://learn.deeplearning.ai/courses/efficiently-serving-llms" target="_blank">Efficiently Serving LLMs</a></li>
                            <li><a href="https://learn.deeplearning.ai/courses/finetuning-large-language-models" target="_blank">Finetuning Large Language Models</a></li>
                        </ul>
                    </li>
                    <li><strong>HuggingFace Documentation</strong> for Transformers and PEFT libraries</li>
                    <li><strong>The Kubernetes Community</strong> for excellent GPU scheduling docs</li>
                </ul>
            </div>

            <h2>Try It Yourself</h2>

            <p>All code, configurations, and deployment manifests are available in my GitHub repository. The README includes:</p>

            <ul>
                <li>Step-by-step setup instructions</li>
                <li>AWS instance configuration</li>
                <li>Kubernetes deployment guides</li>
                <li>Training pipeline documentation</li>
            </ul>

            <p>The fine-tuned model is publicly available on <a href="https://huggingface.co/shettynavisha25/tinyllama-alpaca-finetuned" target="_blank">HuggingFace Hub</a>.</p>

            <div style="text-align: center; margin: 40px 0;">
                <a href="https://github.com/NavishaShetty/LLM-finetuning-on-kubernetes" target="_blank" class="cta-button">View on GitHub</a>
                <a href="https://huggingface.co/shettynavisha25/tinyllama-alpaca-finetuned" target="_blank" class="cta-button">Try the Model</a>
            </div>

            <h2>Final Thoughts</h2>

            <p>This project taught me that building production AI isn't about having the biggest model or the most GPUs‚Äîit's about understanding the full stack. From Kubernetes orchestration to Docker containerization to GPU resource management to checkpoint recovery, every layer matters.</p>

            <p>More importantly, it showed me that curiosity + persistence can take you far. I started knowing almost nothing about fine-tuning or Kubernetes GPU scheduling. I ended with a working, production-grade LLM pipeline that I built, debugged, and deployed myself.</p>

            <p>If you're curious about LLMs, don't just read about them‚Äîbuild something. Break things. Learn from failures. The journey from "I wonder how this works" to "I built this" is worth every debugging session.</p>

            <p><strong>Stay tuned for more adventures in LLMOps.</strong></p>

        </article>

        <footer>
            <p>Have questions about the project? Want to discuss fine-tuning techniques or Kubernetes GPU scheduling?</p>
            <p>Feel free to reach out or check out the GitHub repo. Let's learn together. üöÄ</p>
            <p style="margin-top: 20px; opacity: 0.7;">¬© 2025 | Built with passion for AI and MLOps</p>
        </footer>
    </div>
</body>
</html>